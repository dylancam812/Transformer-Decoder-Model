# Transformer-Decoder-Model
Decoder portion of transformer model made to emulate the structure of gpt4 and other large language models. Uses word embedding, positional encoding, masked multi-head attention, and linear layers to train and ultimately predict the next word in a sequence. Credits/thanks to Andrej Karpathy.
